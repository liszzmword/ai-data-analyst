"""
ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ë¶„ì„ê¸°
ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ìë™ ë¶„ì„í•˜ê³  Geminië¡œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
"""
import google.generativeai as genai
import pandas as pd
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import base64

from config import GOOGLE_API_KEY
from upload_handler import UploadHandler, UploadedFile


# Gemini API ì„¤ì •
genai.configure(api_key=GOOGLE_API_KEY)


@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼"""
    query: str
    data_context: str  # ì‚¬ìš©ëœ ë°ì´í„° ìš”ì•½
    gemini_response: str  # Gemini ì‘ë‹µ
    charts: List[Dict] = None  # ì°¨íŠ¸ ë°ì´í„°
    tables: List[pd.DataFrame] = None  # ê²°ê³¼ í…Œì´ë¸”


class SmartAnalyst:
    """ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” AI ë¶„ì„ê°€"""

    def __init__(self, upload_handler: UploadHandler):
        self.upload_handler = upload_handler
        self.llm = genai.GenerativeModel('gemini-2.5-pro')  # ìµœì‹  Pro ëª¨ë¸
        print("âœ“ ìŠ¤ë§ˆíŠ¸ ë¶„ì„ê¸° ì´ˆê¸°í™” (Gemini 2.5 Pro)")

    def analyze(self, query: str, include_images: bool = True, conversation_context: list = None) -> AnalysisResult:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ì„ ìˆ˜í–‰ (ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì§€ì›)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            include_images: ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€
            conversation_context: ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (ìµœê·¼ 3ê°œ)

        Returns:
            AnalysisResult
        """
        print(f"\n{'='*60}")
        print(f"ğŸ” ì§ˆë¬¸: {query}")
        if conversation_context:
            print(f"ğŸ“ ì´ì „ ëŒ€í™”: {len(conversation_context)}ê°œ")
        print(f"{'='*60}")

        # 1. ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        data_context = self._build_data_context(query, include_images)

        # 2. Gemini ë¶„ì„ (ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        gemini_response = self._generate_analysis(query, data_context, include_images, conversation_context)

        # 3. ê²°ê³¼ í…Œì´ë¸”/ì°¨íŠ¸ ì¶”ì¶œ (í•„ìš”ì‹œ)
        tables, charts = self._extract_results(query, data_context)

        return AnalysisResult(
            query=query,
            data_context=data_context,
            gemini_response=gemini_response,
            charts=charts,
            tables=tables
        )

    def _build_data_context(self, query: str, include_images: bool) -> str:
        """ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ë‹¤ì¤‘ íŒŒì¼ ì¡°ì¸ ì§€ì›"""
        print("ğŸ“Š ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘...")

        context_parts = []
        context_parts.append("=== ì—…ë¡œë“œëœ ë°ì´í„° ===\n")

        # DataFrame ë°ì´í„°
        dataframes = self.upload_handler.get_all_dataframes()
        if dataframes:
            context_parts.append(f"**í…Œì´ë¸” ë°ì´í„°** ({len(dataframes)}ê°œ íŒŒì¼):\n")

            # ì—¬ëŸ¬ íŒŒì¼ì´ ìˆê³  ê±°ë˜ì²˜ ê¸°ì¤€ ì¡°ì¸ ê°€ëŠ¥í•œ ê²½ìš°
            if len(dataframes) > 1:
                joined_df = self._join_dataframes(dataframes)
                if joined_df is not None:
                    context_parts.append(f"\n[í†µí•© ë°ì´í„° (ê±°ë˜ì²˜ ê¸°ì¤€ ì¡°ì¸)]")
                    context_parts.append(f"- ì´ í–‰ ìˆ˜: {len(joined_df):,}")
                    context_parts.append(f"- ì´ ì—´ ìˆ˜: {len(joined_df.columns)}")
                    context_parts.append(f"- ê±°ë˜ì²˜ ìˆ˜: {joined_df['ê±°ë˜ì²˜'].nunique() if 'ê±°ë˜ì²˜' in joined_df.columns else 'N/A'}\n")

                    # ì¡°ì¸ëœ ë°ì´í„°ë¡œ ë¶„ì„
                    relevant_data = self._find_relevant_data(query, joined_df, "í†µí•© ë°ì´í„°")
                    if relevant_data:
                        context_parts.append(f"ê´€ë ¨ ë°ì´í„°:")
                        context_parts.append(relevant_data)

                    # ê°œë³„ íŒŒì¼ ì •ë³´ë„ ê°„ëµíˆ í‘œì‹œ
                    context_parts.append(f"\n**ê°œë³„ íŒŒì¼ ì •ë³´**:")
                    for filename, df in dataframes.items():
                        context_parts.append(f"- {filename}: {len(df):,}í–‰, {len(df.columns)}ì—´")
                else:
                    # ì¡°ì¸ ì‹¤íŒ¨ ì‹œ ê°œë³„ íŒŒì¼ë¡œ ë¶„ì„
                    for filename, df in dataframes.items():
                        context_parts.append(f"\n[{filename}]")
                        context_parts.append(f"- í–‰ ìˆ˜: {len(df):,}")
                        context_parts.append(f"- ì—´: {', '.join(df.columns[:10].tolist())}")

                        relevant_data = self._find_relevant_data(query, df, filename)
                        if relevant_data:
                            context_parts.append(f"\nê´€ë ¨ ë°ì´í„°:")
                            context_parts.append(relevant_data)
            else:
                # íŒŒì¼ì´ 1ê°œì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹
                for filename, df in dataframes.items():
                    context_parts.append(f"\n[{filename}]")
                    context_parts.append(f"- í–‰ ìˆ˜: {len(df):,}")
                    context_parts.append(f"- ì—´: {', '.join(df.columns[:10].tolist())}")

                    relevant_data = self._find_relevant_data(query, df, filename)
                    if relevant_data:
                        context_parts.append(f"\nê´€ë ¨ ë°ì´í„°:")
                        context_parts.append(relevant_data)

        # ì´ë¯¸ì§€ ë°ì´í„°
        if include_images:
            images = [f for f in self.upload_handler.uploaded_files if f.type == 'image']
            if images:
                context_parts.append(f"\n**ì´ë¯¸ì§€** ({len(images)}ê°œ):")
                for img_file in images:
                    context_parts.append(f"- {img_file.name}")

        return "\n".join(context_parts)

    def _join_dataframes(self, dataframes: Dict[str, pd.DataFrame]) -> Optional[pd.DataFrame]:
        """ì—¬ëŸ¬ DataFrameì„ ê±°ë˜ì²˜ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸ (ê±°ë˜ì²˜ì½”ë“œ â†” ê±°ë˜ì²˜ëª… ë§¤í•‘ ì§€ì›)"""
        try:
            # 1. ê±°ë˜ì²˜ ì½”ë“œ â†” ê±°ë˜ì²˜ëª… ë§¤í•‘ í…Œì´ë¸” ìƒì„±
            code_to_name_map = {}
            name_to_code_map = {}

            for filename, df in dataframes.items():
                if 'ê±°ë˜ì²˜ ì½”ë“œ' in df.columns and 'ê±°ë˜ì²˜' in df.columns:
                    for _, row in df[['ê±°ë˜ì²˜ ì½”ë“œ', 'ê±°ë˜ì²˜']].dropna().iterrows():
                        code = str(row['ê±°ë˜ì²˜ ì½”ë“œ']).strip()
                        name = str(row['ê±°ë˜ì²˜']).strip()
                        code_to_name_map[code] = name
                        name_to_code_map[name] = code
                elif 'ê±°ë˜ì²˜ ì½”ë“œ' in df.columns and 'ê±°ë˜ì²˜ëª…' in df.columns:
                    for _, row in df[['ê±°ë˜ì²˜ ì½”ë“œ', 'ê±°ë˜ì²˜ëª…']].dropna().iterrows():
                        code = str(row['ê±°ë˜ì²˜ ì½”ë“œ']).strip()
                        name = str(row['ê±°ë˜ì²˜ëª…']).strip()
                        code_to_name_map[code] = name
                        name_to_code_map[name] = code

            print(f"  â†’ ê±°ë˜ì²˜ ë§¤í•‘: ì½”ë“œ {len(code_to_name_map)}ê°œ, ì´ë¦„ {len(name_to_code_map)}ê°œ")

            # 2. ê±°ë˜ì²˜ ì»¬ëŸ¼ì´ ìˆëŠ” íŒŒì¼ë“¤ë§Œ ì„ íƒ
            joinable_dfs = []
            for filename, df in dataframes.items():
                if 'ê±°ë˜ì²˜' in df.columns or 'ê±°ë˜ì²˜ëª…' in df.columns or 'ê±°ë˜ì²˜ ì½”ë“œ' in df.columns:
                    df_copy = df.copy()

                    # ê±°ë˜ì²˜ ì»¬ëŸ¼ í†µì¼ (ìš°ì„ ìˆœìœ„: ê±°ë˜ì²˜ > ê±°ë˜ì²˜ëª… > ê±°ë˜ì²˜ ì½”ë“œë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜)
                    if 'ê±°ë˜ì²˜' not in df_copy.columns:
                        if 'ê±°ë˜ì²˜ëª…' in df_copy.columns:
                            df_copy['ê±°ë˜ì²˜'] = df_copy['ê±°ë˜ì²˜ëª…']
                        elif 'ê±°ë˜ì²˜ ì½”ë“œ' in df_copy.columns:
                            # ê±°ë˜ì²˜ ì½”ë“œë¥¼ ê±°ë˜ì²˜ëª…ìœ¼ë¡œ ë³€í™˜
                            df_copy['ê±°ë˜ì²˜'] = df_copy['ê±°ë˜ì²˜ ì½”ë“œ'].apply(
                                lambda x: code_to_name_map.get(str(x).strip(), str(x)) if pd.notna(x) else None
                            )
                            print(f"  â†’ {filename}: ê±°ë˜ì²˜ ì½”ë“œ â†’ ê±°ë˜ì²˜ëª… ë³€í™˜")

                    # íŒŒì¼ëª…ì„ prefixë¡œ ì»¬ëŸ¼ëª… ë³€ê²½ (ì¤‘ë³µ ë°©ì§€)
                    file_prefix = filename.replace('.csv', '').replace(' ', '_').replace('(', '').replace(')', '').replace('ver1', '').replace('_1', '')
                    rename_dict = {}
                    important_cols = ['ê±°ë˜ì²˜', 'ê±°ë˜ì²˜ëª…', 'ê±°ë˜ì²˜ ì½”ë“œ', 'ë§¤ì¶œì¼', 'ê±°ë˜ì¼', 'í•©ê³„', 'ì´ íŒë§¤ê¸ˆì•¡', 'ê³µê¸‰ê°€ì•¡', 'ë§ˆì§„ìœ¨', 'ì œí’ˆëª…', 'ì œí’ˆêµ°']

                    for col in df_copy.columns:
                        if col not in important_cols:
                            rename_dict[col] = f"{file_prefix}_{col}"

                    df_copy = df_copy.rename(columns=rename_dict)
                    joinable_dfs.append((filename, df_copy))

            if len(joinable_dfs) < 2:
                return None

            # 3. ì²« ë²ˆì§¸ DataFrameë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì¡°ì¸
            result_df = joinable_dfs[0][1]
            print(f"  â†’ ì¡°ì¸ ì‹œì‘: {joinable_dfs[0][0]} ({len(result_df):,}í–‰)")

            for i in range(1, len(joinable_dfs)):
                filename, df = joinable_dfs[i]
                before_rows = len(result_df)

                # outer joinìœ¼ë¡œ ëª¨ë“  ë°ì´í„° ë³´ì¡´
                result_df = pd.merge(
                    result_df,
                    df,
                    on='ê±°ë˜ì²˜',
                    how='outer',
                    suffixes=('', f'_{i}')
                )

                print(f"  â†’ {filename} ì¡°ì¸: {before_rows:,}í–‰ â†’ {len(result_df):,}í–‰")

            print(f"âœ“ ì¡°ì¸ ì™„ë£Œ: ì´ {len(result_df):,}í–‰, {len(result_df.columns)}ì—´")
            return result_df

        except Exception as e:
            print(f"  âš  ì¡°ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _find_relevant_data(self, query: str, df: pd.DataFrame, filename: str) -> str:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë°ì´í„° ì°¾ê¸° - PANDAS ê³„ì‚° í¬í•¨"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
        keywords = self._extract_keywords(query)

        relevant_parts = []

        # ì»¬ëŸ¼ëª…ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        matching_cols = []
        for col in df.columns:
            if any(keyword in str(col).lower() for keyword in keywords):
                matching_cols.append(col)

        # ===== íŠ¹ì • ê±°ë˜ì²˜ ê²€ìƒ‰ (ìµœìš°ì„ ) =====
        company_name = self._extract_company_name(query, df)
        if company_name:
            calculated_data = self._analyze_specific_company(company_name, df, query)
            if calculated_data:
                relevant_parts.append("=== íŠ¹ì • ê±°ë˜ì²˜ ë¶„ì„ ===")
                relevant_parts.append(calculated_data)
                return "\n".join(relevant_parts)

        # ===== PANDAS ê³„ì‚° ì¶”ê°€ =====
        # ìƒìœ„ Nê°œ ìš”ì²­ (ë§¤ì¶œ ìƒìœ„, ê±°ë˜ì²˜ ìƒìœ„ ë“±)
        if any(word in query for word in ['ìƒìœ„', 'top', 'ë§ì´', 'ë†’ì€', 'ìˆœìœ„']):
            calculated_data = self._calculate_top_n(query, df)
            if calculated_data:
                relevant_parts.append("=== ê³„ì‚°ëœ ê²°ê³¼ (Pandas ì§‘ê³„) ===")
                relevant_parts.append(calculated_data)

        # í•©ê³„/í‰ê·  ìš”ì²­
        elif any(word in query for word in ['í•©ê³„', 'ì´', 'í‰ê· ', 'ì´í•©']):
            calculated_data = self._calculate_aggregates(query, df)
            if calculated_data:
                relevant_parts.append("=== ê³„ì‚°ëœ ê²°ê³¼ (Pandas ì§‘ê³„) ===")
                relevant_parts.append(calculated_data)

        # ì „ì²´ íšŒì‚¬/ê±°ë˜ì²˜ ëª©ë¡ ìš”ì²­
        elif any(word in query for word in ['ì „ì²´', 'ëª¨ë“ ', 'ë¦¬ìŠ¤íŠ¸', 'ëª©ë¡']) and any(word in query for word in ['íšŒì‚¬', 'ê±°ë˜ì²˜', 'ì—…ì²´']):
            if 'ê±°ë˜ì²˜' in df.columns or 'ê±°ë˜ì²˜ëª…' in df.columns:
                col_name = 'ê±°ë˜ì²˜' if 'ê±°ë˜ì²˜' in df.columns else 'ê±°ë˜ì²˜ëª…'
                unique_companies = df[col_name].unique()
                relevant_parts.append(f"\nì „ì²´ ê±°ë˜ì²˜ ëª©ë¡ ({len(unique_companies)}ê°œ):")
                relevant_parts.append(", ".join([str(c) for c in unique_companies[:100]]))  # ìµœëŒ€ 100ê°œ

        # ì¼ë°˜ ë°ì´í„° ìƒ˜í”Œ (ê³„ì‚° ì—†ëŠ” ê²½ìš°)
        if not relevant_parts:
            if matching_cols:
                relevant_parts.append(f"ê´€ë ¨ ì»¬ëŸ¼: {', '.join(matching_cols)}")
                # ë” ë§ì€ ìƒ˜í”Œ ë°ì´í„° (5ê°œ â†’ 20ê°œ)
                sample_df = df[matching_cols].head(20)
                relevant_parts.append(sample_df.to_string())
            else:
                # ì „ì²´ ë°ì´í„° ìƒ˜í”Œ
                relevant_parts.append("ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 20í–‰):")
                relevant_parts.append(df.head(20).to_string())

        return "\n".join(relevant_parts) if relevant_parts else ""

    def _calculate_top_n(self, query: str, df: pd.DataFrame) -> str:
        """ìƒìœ„ Nê°œ ê³„ì‚° (Pandas groupby + sort) - ì „ì²´ ë°ì´í„° ë°˜í™˜"""
        try:
            # N ì¶”ì¶œ (ìƒìœ„ 5ê°œ, Top 10 ë“±)
            n = None  # ì‚¬ìš©ìê°€ ëª…ì‹œí•œ ê²½ìš°ë§Œ ì œí•œ
            for word in query.split():
                if word.isdigit():
                    n = int(word)
                    break

            results = []

            # ê±°ë˜ì²˜ë³„ ë§¤ì¶œ ì§‘ê³„
            if 'ê±°ë˜ì²˜' in df.columns:
                # ë§¤ì¶œ ê´€ë ¨ ìˆ«ì ì»¬ëŸ¼ ì°¾ê¸° (ë‚ ì§œ ì»¬ëŸ¼ ì œì™¸)
                numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
                amount_cols = [col for col in numeric_cols
                             if any(keyword in col for keyword in ['í•©ê³„', 'ê¸ˆì•¡', 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸'])
                             and 'ë²ˆí˜¸' not in col and 'ì½”ë“œ' not in col]

                if amount_cols:
                    for amount_col in amount_cols[:1]:  # ê°€ì¥ ì¤‘ìš”í•œ ì»¬ëŸ¼ 1ê°œë§Œ (í•©ê³„ or ê³µê¸‰ê°€ì•¡)
                        try:
                            # ê±°ë˜ì²˜ë³„ ì§‘ê³„ - ì „ì²´ ë°ì´í„°
                            grouped = df.groupby('ê±°ë˜ì²˜')[amount_col].sum().sort_values(ascending=False)

                            # ì‚¬ìš©ìê°€ Nì„ ëª…ì‹œí•œ ê²½ìš°ë§Œ ì œí•œ
                            if n:
                                grouped = grouped.head(n)
                                results.append(f"\n[ê±°ë˜ì²˜ë³„ {amount_col} ìƒìœ„ {n}ê°œ]")
                            else:
                                # ì „ì²´ ê±°ë˜ì²˜ (ë„ˆë¬´ ë§ìœ¼ë©´ ìƒìœ„ 50ê°œ)
                                if len(grouped) > 50:
                                    results.append(f"\n[ê±°ë˜ì²˜ë³„ {amount_col} ì „ì²´ (ì´ {len(grouped)}ê°œ ì¤‘ ìƒìœ„ 50ê°œ)]")
                                    grouped = grouped.head(50)
                                else:
                                    results.append(f"\n[ê±°ë˜ì²˜ë³„ {amount_col} ì „ì²´ ({len(grouped)}ê°œ)]")

                            results.append("ìˆœìœ„ | ê±°ë˜ì²˜ | ê¸ˆì•¡")
                            results.append("-" * 50)

                            for rank, (company, value) in enumerate(grouped.items(), 1):
                                results.append(f"{rank}ìœ„ | {company} | {value:,.0f}")
                        except Exception as e:
                            print(f"ì»¬ëŸ¼ {amount_col} ê³„ì‚° ì˜¤ë¥˜: {e}")
                            continue

            # ì œí’ˆë³„ ì§‘ê³„
            if 'í’ˆëª©ëª…' in df.columns or 'ì œí’ˆëª…' in df.columns or 'ê±°ë˜ ì œí’ˆëª…' in df.columns:
                product_col = 'í’ˆëª©ëª…' if 'í’ˆëª©ëª…' in df.columns else ('ì œí’ˆëª…' if 'ì œí’ˆëª…' in df.columns else 'ê±°ë˜ ì œí’ˆëª…')
                amount_cols = [col for col in df.select_dtypes(include=['number']).columns
                             if any(keyword in col for keyword in ['í•©ê³„', 'ê¸ˆì•¡', 'ìˆ˜ëŸ‰'])
                             and 'ë²ˆí˜¸' not in col and 'ì½”ë“œ' not in col]

                if amount_cols:
                    try:
                        grouped = df.groupby(product_col)[amount_cols[0]].sum().sort_values(ascending=False)

                        if n:
                            grouped = grouped.head(n)
                            results.append(f"\n[{product_col}ë³„ {amount_cols[0]} ìƒìœ„ {n}ê°œ]")
                        else:
                            if len(grouped) > 30:
                                results.append(f"\n[{product_col}ë³„ {amount_cols[0]} ìƒìœ„ 30ê°œ (ì´ {len(grouped)}ê°œ)]")
                                grouped = grouped.head(30)
                            else:
                                results.append(f"\n[{product_col}ë³„ {amount_cols[0]} ì „ì²´ ({len(grouped)}ê°œ)]")

                        results.append("ìˆœìœ„ | ì œí’ˆ | ê¸ˆì•¡")
                        results.append("-" * 50)

                        for rank, (product, value) in enumerate(grouped.items(), 1):
                            results.append(f"{rank}ìœ„ | {product} | {value:,.0f}")
                    except:
                        pass

            return "\n".join(results) if results else ""

        except Exception as e:
            print(f"ìƒìœ„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return ""

    def _calculate_aggregates(self, query: str, df: pd.DataFrame) -> str:
        """í•©ê³„/í‰ê·  ê³„ì‚°"""
        try:
            results = []

            # ìˆ«ì ì»¬ëŸ¼ë§Œ ì„ íƒ
            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

            # ë²ˆí˜¸, ì½”ë“œ ê°™ì€ ì˜ë¯¸ì—†ëŠ” ì»¬ëŸ¼ ì œì™¸
            exclude_keywords = ['ë²ˆí˜¸', 'ì½”ë“œ', 'id', 'index']
            meaningful_cols = [col for col in numeric_cols
                             if not any(keyword in col.lower() for keyword in exclude_keywords)]

            if meaningful_cols:
                results.append("[ì „ì²´ ë°ì´í„° ì§‘ê³„]")
                results.append("ì»¬ëŸ¼ | í•©ê³„ | í‰ê·  | ìµœëŒ€ | ìµœì†Œ")
                results.append("-" * 70)

                for col in meaningful_cols[:10]:  # ìµœëŒ€ 10ê°œ
                    total = df[col].sum()
                    mean = df[col].mean()
                    max_val = df[col].max()
                    min_val = df[col].min()

                    results.append(f"{col} | {total:,.0f} | {mean:,.1f} | {max_val:,.0f} | {min_val:,.0f}")

            # ê±°ë˜ì²˜ë³„ ì§‘ê³„ (ìˆëŠ” ê²½ìš°)
            if 'ê±°ë˜ì²˜' in df.columns and meaningful_cols:
                results.append("\n[ê±°ë˜ì²˜ë³„ ì§‘ê³„ (ìƒìœ„ 10ê°œ)]")

                for col in meaningful_cols[:2]:
                    try:
                        grouped = df.groupby('ê±°ë˜ì²˜')[col].agg(['sum', 'mean', 'count']).nlargest(10, 'sum')

                        results.append(f"\n{col}:")
                        results.append("ê±°ë˜ì²˜ | í•©ê³„ | í‰ê·  | ê±´ìˆ˜")
                        results.append("-" * 70)

                        for company, row in grouped.iterrows():
                            results.append(f"{company} | {row['sum']:,.0f} | {row['mean']:,.1f} | {int(row['count'])}")
                    except:
                        continue

            return "\n".join(results) if results else ""

        except Exception as e:
            print(f"ì§‘ê³„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return ""

    def _extract_company_name(self, query: str, df: pd.DataFrame) -> Optional[str]:
        """ì§ˆë¬¸ì—ì„œ ê±°ë˜ì²˜ëª… ì¶”ì¶œ"""
        if 'ê±°ë˜ì²˜' not in df.columns and 'ê±°ë˜ì²˜ëª…' not in df.columns:
            return None

        col_name = 'ê±°ë˜ì²˜' if 'ê±°ë˜ì²˜' in df.columns else 'ê±°ë˜ì²˜ëª…'
        all_companies = df[col_name].unique()

        # ì§ˆë¬¸ì—ì„œ ì‹¤ì œ ê±°ë˜ì²˜ëª… ì°¾ê¸°
        for company in all_companies:
            if pd.notna(company) and str(company) in query:
                return str(company)

        return None

    def _analyze_specific_company(self, company_name: str, df: pd.DataFrame, query: str) -> str:
        """íŠ¹ì • ê±°ë˜ì²˜ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„"""
        try:
            col_name = 'ê±°ë˜ì²˜' if 'ê±°ë˜ì²˜' in df.columns else 'ê±°ë˜ì²˜ëª…'

            # í•´ë‹¹ ê±°ë˜ì²˜ ë°ì´í„° í•„í„°ë§
            company_data = df[df[col_name] == company_name].copy()

            if len(company_data) == 0:
                return f"'{company_name}' ê±°ë˜ì²˜ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            results = []
            results.append(f"\n[{company_name} ê±°ë˜ì²˜ ìƒì„¸ ë¶„ì„]")
            results.append(f"ì´ ê±°ë˜ ê±´ìˆ˜: {len(company_data):,}ê±´\n")

            # ìˆ«ì ì»¬ëŸ¼ ì§‘ê³„
            numeric_cols = company_data.select_dtypes(include=['number']).columns.tolist()
            exclude_keywords = ['ë²ˆí˜¸', 'ì½”ë“œ', 'id', 'index']
            meaningful_cols = [col for col in numeric_cols
                             if not any(keyword in col.lower() for keyword in exclude_keywords)]

            if meaningful_cols:
                results.append("**ì£¼ìš” ìˆ˜ì¹˜ ì§‘ê³„**:")
                results.append("í•­ëª© | í•©ê³„ | í‰ê·  | ìµœëŒ€ | ìµœì†Œ")
                results.append("-" * 70)

                for col in meaningful_cols[:10]:
                    total = company_data[col].sum()
                    mean = company_data[col].mean()
                    max_val = company_data[col].max()
                    min_val = company_data[col].min()
                    results.append(f"{col} | {total:,.0f} | {mean:,.1f} | {max_val:,.0f} | {min_val:,.0f}")

            # ì—°ë„ë³„ ë¶„ì„ (ë§¤ì¶œì¼ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
            if 'ë§¤ì¶œì¼' in company_data.columns or 'ê±°ë˜ì¼' in company_data.columns or 'ì¼ì' in company_data.columns:
                date_col = 'ë§¤ì¶œì¼' if 'ë§¤ì¶œì¼' in company_data.columns else ('ê±°ë˜ì¼' if 'ê±°ë˜ì¼' in company_data.columns else 'ì¼ì')

                try:
                    # ë‚ ì§œ íŒŒì‹±
                    company_data[date_col] = pd.to_datetime(company_data[date_col], errors='coerce')
                    company_data['ì—°ë„'] = company_data[date_col].dt.year

                    # ì—°ë„ë³„ ì§‘ê³„
                    if 'í•©ê³„' in company_data.columns:
                        yearly = company_data.groupby('ì—°ë„')['í•©ê³„'].agg(['sum', 'count']).sort_index()

                        results.append("\n**ì—°ë„ë³„ ë§¤ì¶œ ì¶”ì´**:")
                        results.append("ì—°ë„ | ë§¤ì¶œ í•©ê³„ | ê±°ë˜ ê±´ìˆ˜")
                        results.append("-" * 50)

                        for year, row in yearly.iterrows():
                            results.append(f"{int(year)}ë…„ | {row['sum']:,.0f} | {int(row['count'])}ê±´")
                except:
                    pass

            # ì œí’ˆë³„ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
            product_cols = ['í’ˆëª©ëª…', 'ì œí’ˆëª…', 'ê±°ë˜ ì œí’ˆëª…']
            product_col = None
            for col in product_cols:
                if col in company_data.columns:
                    product_col = col
                    break

            if product_col and 'í•©ê³„' in company_data.columns:
                product_sales = company_data.groupby(product_col)['í•©ê³„'].sum().sort_values(ascending=False).head(10)

                results.append(f"\n**ì£¼ìš” ê±°ë˜ ì œí’ˆ (ìƒìœ„ 10ê°œ)**:")
                results.append("ì œí’ˆëª… | ë§¤ì¶œ í•©ê³„")
                results.append("-" * 50)

                for product, sales in product_sales.items():
                    results.append(f"{product} | {sales:,.0f}")

            # ì „ì²´ ë°ì´í„° ìƒ˜í”Œ (ìµœê·¼ 10ê±´)
            results.append("\n**ìµœê·¼ ê±°ë˜ ë‚´ì—­ (10ê±´)**:")
            sample_data = company_data.tail(10)
            results.append(sample_data.to_string(index=False))

            return "\n".join(results)

        except Exception as e:
            print(f"ê±°ë˜ì²˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return f"'{company_name}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    def _extract_keywords(self, query: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±°)
        stopwords = ['ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€',
                     'í•´', 'í•´ì£¼', 'í•´ì¤˜', 'ì•Œë ¤', 'ì•Œë ¤ì¤˜', 'ë³´ì—¬', 'ë³´ì—¬ì¤˜', 'ë¶„ì„', 'ì„¤ëª…']

        words = query.replace('?', '').replace(',', '').split()
        keywords = [w.lower() for w in words if w not in stopwords and len(w) > 1]

        return keywords

    def _generate_analysis(self, query: str, data_context: str, include_images: bool, conversation_context: list = None) -> str:
        """Geminië¡œ ë¶„ì„ ìƒì„± (AI íŒë‹¨ ê°•í™” + ëŒ€í™” ì»¨í…ìŠ¤íŠ¸)"""
        print("ğŸ¤– Gemini ë¶„ì„ ì¤‘...")

        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ multimodal í”„ë¡¬í”„íŠ¸
        if include_images:
            images = [f for f in self.upload_handler.uploaded_files if f.type == 'image']
            if images:
                return self._generate_multimodal_analysis(query, data_context, images)

        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_str = ""
        if conversation_context and len(conversation_context) > 0:
            context_str = "\n**ì´ì „ ëŒ€í™” ë‚´ì—­** (ì°¸ê³ ìš©):\n"
            for i, ctx in enumerate(conversation_context[-3:], 1):  # ìµœê·¼ 3ê°œë§Œ
                context_str += f"{i}. ì§ˆë¬¸: {ctx['query']}\n"
                context_str += f"   ë‹µë³€: {ctx['response'][:200]}...\n\n"  # ë‹µë³€ì€ 200ìê¹Œì§€ë§Œ

        # í…ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ **ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
{context_str}
**í˜„ì¬ ì§ˆë¬¸**: {query}

**ì—…ë¡œë“œëœ ë°ì´í„° ì •ë³´**:
{data_context}

**ë‹µë³€ ì‘ì„± ê°€ì´ë“œ**:
1. **ë°ì´í„° ìš”ì•½**: ì—…ë¡œë“œëœ ë°ì´í„°ì˜ í•µì‹¬ ë‚´ìš©
2. **ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€**: êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ í•¨ê»˜ ëª…í™•íˆ ë‹µë³€
3. **ì¸ì‚¬ì´íŠ¸ ë° AI íŒë‹¨**:
   - ë°ì´í„°ì—ì„œ ë°œê²¬í•œ ì¤‘ìš”í•œ íŒ¨í„´/íŠ¹ì§•
   - **ì—°í‰ê·  ì„±ì¥ë¥  (CAGR)**: ì—°ë„ë³„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì„±ì¥ë¥  ê³„ì‚° ê³µì‹ = ((ìµœì¢…ë…„ë„ê°’/ì´ˆê¸°ë…„ë„ê°’)^(1/(ë…„ìˆ˜-1)) - 1) Ã— 100
   - **ê±°ë˜ ëŠê¸¸ ìœ„í—˜ ë¶„ì„**: ìµœê·¼ 3ê°œì›” ë§¤ì¶œì´ ì´ì „ 3ê°œì›” ëŒ€ë¹„ 50% ì´ìƒ ê°ì†Œí•œ ê±°ë˜ì²˜, ë˜ëŠ” ê±°ë˜ ë¹ˆë„ê°€ ê¸‰ê²©íˆ ì¤„ì–´ë“  ê±°ë˜ì²˜
   - **ê³ ê° ë“±ê¸‰ë³„ íŠ¹ì§•**: R(Recency), F(Frequency), M(Monetary) ê¸°ì¤€ ì¶©ì„±ê³ ê°, ì ì¬ê³ ê°, ìœ„í—˜ê³ ê° ë¶„ë¥˜
   - **ì œí’ˆêµ°ë³„ íŠ¸ë Œë“œ**: íŠ¹ì • ì œí’ˆêµ° ë§¤ì¶œ ì¦ê°€/ê°ì†Œ ì¶”ì„¸
4. **ì œì•ˆ ë° ì¡°ì–¸**:
   - ì˜ì‚¬ê²°ì •ì— ë„ì›€ë˜ëŠ” êµ¬ì²´ì  ì¡°ì–¸
   - ì£¼ì˜ê°€ í•„ìš”í•œ ê±°ë˜ì²˜/ì œí’ˆ
   - ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•œ ë¶€ë¶„

**ì¤‘ìš” - ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™**:
- í•œêµ­ì–´ë¡œ ë‹µë³€
- **ìœ„ì— ì œê³µëœ "ê³„ì‚°ëœ ê²°ê³¼" ì„¹ì…˜ì˜ ì‹¤ì œ íšŒì‚¬ëª…/ì œí’ˆëª…ë§Œ ì‚¬ìš©í•  ê²ƒ**
- **ì ˆëŒ€ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íšŒì‚¬ëª…ì„ ì§€ì–´ë‚´ì§€ ë§ ê²ƒ (ì˜ˆ: "ì£¼ì‹íšŒì‚¬ ê°€ë‚˜ë‹¤ë¼", "ë² ìŠ¤íŠ¸ì¶œíŒ" ê°™ì€ ê°€ì§œ ì´ë¦„ ê¸ˆì§€)**
- **ì¤‘êµ­ì–´ ê¸°ì—…ëª…ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì§€ ë§ ê²ƒ (ì˜ˆ: "ì“°ì´¨ì‰¬í™ OPTO-ì „ì"ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)**
- êµ¬ì²´ì ì¸ ìˆ«ì/ì‚¬ì‹¤ë§Œ ì–¸ê¸‰
- ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë°ì´í„°ì— ì—†ìŒ"ì´ë¼ê³  ëª…ì‹œ
- ì œê³µëœ pandas ê³„ì‚° ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
- ê±°ë˜ì²˜ ì½”ë“œê°€ ì£¼ì–´ì§€ë©´ ë°˜ë“œì‹œ ê±°ë˜ì²˜ëª…ìœ¼ë¡œ ë³€í™˜í•´ì„œ ë‹µë³€
- NULL/ë¹ˆ ê°’ì€ "ë°ì´í„° ì—†ìŒ" ë˜ëŠ” "-"ë¡œ í‘œì‹œ

ë‹µë³€:"""

        try:
            response = self.llm.generate_content(prompt)
            return response.text
        except Exception as e:
            print(f"âœ— Gemini ì˜¤ë¥˜: {e}")
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n\në°ì´í„° ì»¨í…ìŠ¤íŠ¸:\n{data_context}"

    def _generate_multimodal_analysis(
        self, query: str, data_context: str, images: List[UploadedFile]
    ) -> str:
        """ì´ë¯¸ì§€ í¬í•¨ ë©€í‹°ëª¨ë‹¬ ë¶„ì„"""
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í¬í•¨ ë¶„ì„ ({len(images)}ê°œ)")

        # ì´ë¯¸ì§€ë¥¼ Geminiê°€ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
        image_parts = []
        for img_file in images[:5]:  # ìµœëŒ€ 5ê°œ
            # base64 â†’ bytes
            image_bytes = base64.b64decode(img_file.content)
            image_parts.append({
                'mime_type': 'image/jpeg',
                'data': image_bytes
            })

        prompt = f"""ë‹¹ì‹ ì€ **ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë°ì´í„°(í…Œì´ë¸” + ì´ë¯¸ì§€/ì°¨íŠ¸)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸**: {query}

**ì—…ë¡œë“œëœ í…Œì´ë¸” ë°ì´í„°**:
{data_context}

**ì´ë¯¸ì§€/ì°¨íŠ¸**: {len(images)}ê°œ ì œê³µë¨

**ë‹µë³€ ì‘ì„± ê°€ì´ë“œ**:
1. **ì´ë¯¸ì§€ ë¶„ì„**: ì°¨íŠ¸/ê·¸ë˜í”„ê°€ ë³´ì—¬ì£¼ëŠ” í•µì‹¬ ë‚´ìš©
2. **ë°ì´í„° í•´ì„**: í…Œì´ë¸” ë°ì´í„°ì™€ ì´ë¯¸ì§€ë¥¼ ì¢…í•© ë¶„ì„
3. **ì¸ì‚¬ì´íŠ¸**: ë°œê²¬í•œ íŒ¨í„´/íŠ¸ë Œë“œ/ì´ìƒì¹˜
4. **ì œì•ˆ**: ì˜ì‚¬ê²°ì •ì— ë„ì›€ë˜ëŠ” ì¡°ì–¸

**ì¤‘ìš”**:
- í•œêµ­ì–´ë¡œ ë‹µë³€
- ì´ë¯¸ì§€ì˜ êµ¬ì²´ì  ë‚´ìš© ì–¸ê¸‰ (ì˜ˆ: "ì°¨íŠ¸ì—ì„œ 2024ë…„ ë§¤ì¶œì´ ê¸‰ì¦")
- í…Œì´ë¸” ë°ì´í„°ì™€ ì´ë¯¸ì§€ë¥¼ ì—°ê²°í•˜ì—¬ í•´ì„

ë‹µë³€:"""

        try:
            # Geminiì— ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì „ì†¡
            content_parts = [prompt] + image_parts
            response = self.llm.generate_content(content_parts)
            return response.text
        except Exception as e:
            print(f"âœ— ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì˜¤ë¥˜: {e}")
            # Fallback: í…ìŠ¤íŠ¸ë§Œ ë¶„ì„
            return self._generate_analysis(query, data_context, include_images=False)

    def _extract_results(self, query: str, data_context: str) -> tuple:
        """ê²°ê³¼ í…Œì´ë¸”/ì°¨íŠ¸ ì¶”ì¶œ"""
        # í†µê³„ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš°
        tables = []
        charts = []

        dataframes = self.upload_handler.get_all_dataframes()

        # ê°„ë‹¨í•œ ì§‘ê³„ ìˆ˜í–‰
        if any(word in query for word in ['ìƒìœ„', 'Top', 'ìˆœìœ„']):
            for filename, df in dataframes.items():
                numeric_cols = df.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    # ì²« ë²ˆì§¸ ìˆ«ì ì»¬ëŸ¼ ê¸°ì¤€ ì •ë ¬
                    sort_col = numeric_cols[0]
                    top_df = df.nlargest(10, sort_col)
                    tables.append(top_df)
                    break

        return tables, charts


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("="*60)
    print("ìŠ¤ë§ˆíŠ¸ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("="*60)

    from upload_handler import UploadHandler

    # ì—…ë¡œë“œ í•¸ë“¤ëŸ¬ ìƒì„±
    handler = UploadHandler()

    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    test_csv = "/Users/inseoplee/Desktop/rag_Test/sales data.csv"
    with open(test_csv, 'rb') as f:
        file_bytes = f.read()
        uploaded = handler.process_upload(file_bytes, "sales data.csv")
        handler.add_file(uploaded)

    # ë¶„ì„ê¸° ìƒì„±
    analyst = SmartAnalyst(handler)

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    result = analyst.analyze("ë§¤ì¶œ ìƒìœ„ 5ê°œ í•­ëª©ì„ ë¶„ì„í•˜ê³  ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”")

    print(f"\n{'='*60}")
    print("ë¶„ì„ ê²°ê³¼")
    print(f"{'='*60}")
    print(result.gemini_response)
