"""
ì§ˆì˜ ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
"""
import re
import pandas as pd
from typing import List, Tuple, Dict
import google.generativeai as genai

from config import GOOGLE_API_KEY, STAT_KEYWORDS
from data_loader import Document, DataLoader
from vector_store import VectorStore


# Google Gemini API ì„¤ì •
genai.configure(api_key=GOOGLE_API_KEY)


class QueryProcessor:
    """ì§ˆì˜ë¥¼ ì²˜ë¦¬í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, vector_store: VectorStore, data_loader: DataLoader):
        self.vector_store = vector_store
        self.data_loader = data_loader
        self.llm = genai.GenerativeModel('gemini-2.5-flash')

    def process_query(
            self,
            query: str,
            top_k: int = 5,
            dataset_filter: str = "ì „ì²´"
    ) -> Dict:
        """
        ì§ˆì˜ë¥¼ ì²˜ë¦¬í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            dataset_filter: ë°ì´í„°ì…‹ í•„í„°

        Returns:
            {
                'answer': ë‹µë³€ í…ìŠ¤íŠ¸,
                'sources': [(doc, score), ...],
                'statistics': DataFrame or None,
                'is_statistical': bool
            }
        """
        print(f"\nğŸ” ì§ˆì˜ ì²˜ë¦¬ ì¤‘: {query}")

        # 1. í†µê³„ì„± ì§ˆì˜ ì—¬ë¶€ í™•ì¸
        is_statistical = self._is_statistical_query(query)
        print(f"  í†µê³„ì„± ì§ˆì˜: {is_statistical}")

        # 2. RAG ê²€ìƒ‰
        sources = self.vector_store.search(query, top_k=top_k, dataset_filter=dataset_filter)
        print(f"  ê²€ìƒ‰ ê²°ê³¼: {len(sources)} ê°œ ë¬¸ì„œ")

        # 3. í†µê³„ ê³„ì‚° (í•„ìš”ì‹œ)
        statistics_df = None
        statistics_text = ""

        if is_statistical:
            statistics_df, statistics_text = self._calculate_statistics(query, sources, dataset_filter)

        # 4. ë‹µë³€ ìƒì„±
        answer = self._generate_answer(query, sources, statistics_text)

        return {
            'answer': answer,
            'sources': sources,
            'statistics': statistics_df,
            'is_statistical': is_statistical
        }

    def _is_statistical_query(self, query: str) -> bool:
        """
        í†µê³„ì„± ì§ˆì˜ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.

        Args:
            query: ì§ˆì˜ í…ìŠ¤íŠ¸

        Returns:
            í†µê³„ì„± ì§ˆì˜ ì—¬ë¶€
        """
        return any(keyword in query for keyword in STAT_KEYWORDS)

    def _calculate_statistics(
            self,
            query: str,
            sources: List[Tuple[Document, float]],
            dataset_filter: str
    ) -> Tuple[pd.DataFrame, str]:
        """
        í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            query: ì§ˆì˜ í…ìŠ¤íŠ¸
            sources: ê²€ìƒ‰ ê²°ê³¼
            dataset_filter: ë°ì´í„°ì…‹ í•„í„°

        Returns:
            (DataFrame, í†µê³„ ì„¤ëª… í…ìŠ¤íŠ¸)
        """
        print("  ğŸ“Š í†µê³„ ê³„ì‚° ì¤‘...")

        # ê´€ë ¨ ë°ì´í„°ì…‹ ê²°ì •
        if dataset_filter != "ì „ì²´":
            datasets = [dataset_filter]
        else:
            # sourcesì—ì„œ ì–¸ê¸‰ëœ ë°ì´í„°ì…‹ ì¶”ì¶œ
            datasets = list(set(doc.metadata.get('dataset') for doc, _ in sources))

        statistics_parts = []

        for dataset_name in datasets:
            df = self.data_loader.get_dataframe(dataset_name)
            if df is None:
                continue

            # ê°„ë‹¨í•œ í†µê³„ ê³„ì‚°
            stats_text = self._compute_simple_stats(query, df, dataset_name)
            if stats_text:
                statistics_parts.append(stats_text)

        statistics_text = "\n\n".join(statistics_parts) if statistics_parts else ""

        return None, statistics_text

    def _compute_simple_stats(self, query: str, df: pd.DataFrame, dataset_name: str) -> str:
        """
        ê°„ë‹¨í•œ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            query: ì§ˆì˜
            df: DataFrame
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„

        Returns:
            í†µê³„ í…ìŠ¤íŠ¸
        """
        stats_lines = []
        stats_lines.append(f"### {dataset_name} í†µê³„")

        # ê¸°ë³¸ í†µê³„
        stats_lines.append(f"- ì´ í–‰ ìˆ˜: {len(df):,}")

        # ê±°ë˜ì²˜ì½”ë“œ(B-2) ê¸°ì¤€ í†µê³„
        if 'B-2' in df.columns:
            unique_clients = df['B-2'].nunique()
            stats_lines.append(f"- ê³ ìœ  ê±°ë˜ì²˜ ìˆ˜: {unique_clients:,}")

        # ê±°ë˜ì²˜ëª…(B-1) ê¸°ì¤€ Top 5
        if 'B-1' in df.columns and 'í•©ê³„' in query or 'Top' in query or 'ìƒìœ„' in query:
            top_clients = df['B-1'].value_counts().head(5)
            stats_lines.append(f"\nê±°ë˜ì²˜ë³„ ê±´ìˆ˜ Top 5:")
            for i, (client, count) in enumerate(top_clients.items(), 1):
                stats_lines.append(f"  {i}. {client}: {count:,}ê±´")

        # ìˆ«ì ì»¬ëŸ¼ì— ëŒ€í•œ ì§‘ê³„
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            # J-6, J-7, J-8 ê°™ì€ ë§¤ì¶œ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
            revenue_cols = [col for col in numeric_cols if col in ['J-6', 'J-7', 'J-8']]

            if revenue_cols and ('ë§¤ì¶œ' in query or 'í•©ê³„' in query or 'ì´' in query):
                stats_lines.append(f"\nì£¼ìš” ìˆ«ì ì»¬ëŸ¼ í•©ê³„:")
                for col in revenue_cols[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                    total = df[col].sum()
                    if not pd.isna(total) and total > 0:
                        stats_lines.append(f"  - {col}: {total:,.0f}")

        return "\n".join(stats_lines)

    def _generate_answer(
            self,
            query: str,
            sources: List[Tuple[Document, float]],
            statistics_text: str
    ) -> str:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            sources: ê²€ìƒ‰ ê²°ê³¼
            statistics_text: í†µê³„ í…ìŠ¤íŠ¸

        Returns:
            ë‹µë³€ í…ìŠ¤íŠ¸
        """
        print("  ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []

        if statistics_text:
            context_parts.append("=== í†µê³„ ì •ë³´ ===")
            context_parts.append(statistics_text)
            context_parts.append("")

        context_parts.append("=== ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ ===")
        for i, (doc, score) in enumerate(sources, 1):
            context_parts.append(f"\n[ë¬¸ì„œ {i}] (ìœ ì‚¬ë„: {score:.4f})")
            context_parts.append(f"ì¶œì²˜: {doc.metadata['source']}")
            # í…ìŠ¤íŠ¸ì˜ ì²˜ìŒ 500ìë§Œ ì‚¬ìš©
            text_preview = doc.text[:500]
            context_parts.append(text_preview)

        context = "\n".join(context_parts)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ë¬¸ì„œ ë° í†µê³„:
{context}

ë‹µë³€ ì‘ì„± ì§€ì¹¨:
1. ì œê³µëœ ë¬¸ì„œì˜ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì‚¬ì‹¤ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ì¶œì²˜(ë¬¸ì„œ ë²ˆí˜¸)ë¥¼ í‘œì‹œí•˜ì„¸ìš”.
4. í†µê³„ ì •ë³´ê°€ ìˆë‹¤ë©´ í‘œ í˜•íƒœë¡œ ì •ë¦¬í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
5. ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ì—†ë‹¤ë©´ ì†”ì§íˆ ë§ì”€í•´ì£¼ì„¸ìš”.
6. ê·¼ê±°ê°€ ëª…í™•í•˜ì§€ ì•Šì€ ì¶”ì¸¡ì€ í•˜ì§€ ë§ˆì„¸ìš”.

ë‹µë³€:"""

        try:
            # Gemini API í˜¸ì¶œ
            response = self.llm.generate_content(prompt)
            answer = response.text

            return answer

        except Exception as e:
            print(f"âœ— ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def format_sources(self, sources: List[Tuple[Document, float]]) -> str:
        """
        ì¶œì²˜ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

        Args:
            sources: ê²€ìƒ‰ ê²°ê³¼

        Returns:
            í¬ë§·íŒ…ëœ ì¶œì²˜ í…ìŠ¤íŠ¸
        """
        if not sources:
            return "ì¶œì²˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

        source_parts = []
        source_parts.append("## ğŸ“š ê·¼ê±° ë° ì¶œì²˜")
        source_parts.append("")

        for i, (doc, score) in enumerate(sources, 1):
            source_parts.append(f"### [{i}] {doc.metadata['source']} (ìœ ì‚¬ë„: {score:.4f})")

            # í•µì‹¬ ë©”íƒ€ë°ì´í„° í‘œì‹œ
            metadata = doc.metadata
            if 'ê±°ë˜ì²˜ëª…' in metadata:
                source_parts.append(f"- ê±°ë˜ì²˜: {metadata['ê±°ë˜ì²˜ëª…']}")
            if 'ê±°ë˜ì²˜ì½”ë“œ' in metadata:
                source_parts.append(f"- ê±°ë˜ì²˜ì½”ë“œ: {metadata['ê±°ë˜ì²˜ì½”ë“œ']}")
            if 'ë‚ ì§œ' in metadata:
                dates = metadata['ë‚ ì§œ']
                if isinstance(dates, list) and dates:
                    source_parts.append(f"- ë‚ ì§œ: {dates[0]}")

            # ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
            preview = doc.text[:300].replace('\n', ' ')
            source_parts.append(f"\n```\n{preview}...\n```\n")

        return "\n".join(source_parts)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    from data_loader import DataLoader
    from vector_store import VectorStore

    print("=== ì§ˆì˜ ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸ ===")

    # ë°ì´í„° ë° ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    data_loader = DataLoader()
    documents = data_loader.load_all_data()

    vector_store = VectorStore()
    vector_store.build_index(documents)

    # ì§ˆì˜ ì²˜ë¦¬ê¸° ìƒì„±
    processor = QueryProcessor(vector_store, data_loader)

    # í…ŒìŠ¤íŠ¸ ì§ˆì˜
    test_query = "í•œêµ­ì¼€ë¯¸ì¹¼ìƒì‚¬ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"

    result = processor.process_query(test_query, top_k=3)

    print(f"\nì§ˆë¬¸: {test_query}")
    print(f"\në‹µë³€:\n{result['answer']}")
    print(f"\n{processor.format_sources(result['sources'])}")
