"""
ì„ë² ë”© ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
"""
import os
import pickle
import numpy as np
from typing import List, Tuple
from sentence_transformers import SentenceTransformer
import faiss

from config import EMBEDDING_MODEL_NAME, VECTOR_STORE_DIR
from data_loader import Document


class VectorStore:
    """ë²¡í„° ìŠ¤í† ì–´ í´ë˜ìŠ¤ (FAISS ê¸°ë°˜)"""

    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME):
        self.model_name = model_name
        self.model = None
        self.index = None
        self.documents = []
        self.dimension = None

        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        self.index_path = VECTOR_STORE_DIR / "faiss_index.bin"
        self.docs_path = VECTOR_STORE_DIR / "documents.pkl"
        self.config_path = VECTOR_STORE_DIR / "config.pkl"

        # ë²¡í„° ìŠ¤í† ì–´ ë””ë ‰í† ë¦¬ ìƒì„±
        VECTOR_STORE_DIR.mkdir(exist_ok=True, parents=True)

    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.model is None:
            print(f"\nğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            self.dimension = self.model.get_sentence_embedding_dimension()
            print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.dimension})")

    def build_index(self, documents: List[Document], force_rebuild: bool = False):
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            force_rebuild: ê°•ì œë¡œ ì¬ìƒì„± ì—¬ë¶€
        """
        # ìºì‹œëœ ì¸ë±ìŠ¤ í™•ì¸
        if not force_rebuild and self._load_from_cache():
            print("âœ“ ìºì‹œëœ ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            return

        print(f"\nğŸ”¨ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)})")

        # ëª¨ë¸ ë¡œë“œ
        self._load_model()

        # ë¬¸ì„œ ì €ì¥
        self.documents = documents

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [doc.text for doc in documents]

        # ì„ë² ë”© ìƒì„±
        print("  ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.model.encode(
            texts,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´ ì •ê·œí™”
        )

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        print("  FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product (ì •ê·œí™”ëœ ë²¡í„°ë¼ë©´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        self.index.add(embeddings.astype('float32'))

        print(f"âœ“ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (ì´ {self.index.ntotal} ë²¡í„°)")

        # ìºì‹œ ì €ì¥
        self._save_to_cache()

    def search(self, query: str, top_k: int = 5, dataset_filter: str = "ì „ì²´") -> List[Tuple[Document, float]]:
        """
        ì¿¼ë¦¬ì— ëŒ€í•´ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            dataset_filter: ë°ì´í„°ì…‹ í•„í„° ("ì „ì²´", "ê±°ë˜ì²˜", "ë§¤ì¶œ", "ì˜ì—…ì¼ì§€")

        Returns:
            [(Document, score), ...] ë¦¬ìŠ¤íŠ¸
        """
        if self.index is None or self.model is None:
            print("âœ— ë²¡í„° ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.model.encode(
            [query],
            convert_to_numpy=True,
            normalize_embeddings=True
        ).astype('float32')

        # ê²€ìƒ‰ (ë” ë§ì´ ê°€ì ¸ì˜¨ í›„ í•„í„°ë§)
        search_k = min(top_k * 10, self.index.ntotal)
        distances, indices = self.index.search(query_embedding, search_k)

        # ê²°ê³¼ í•„í„°ë§ ë° ì •ë¦¬
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx >= len(self.documents):
                continue

            doc = self.documents[idx]

            # ë°ì´í„°ì…‹ í•„í„° ì ìš©
            if dataset_filter != "ì „ì²´":
                if doc.metadata.get("dataset") != dataset_filter:
                    continue

            score = float(dist)
            results.append((doc, score))

            if len(results) >= top_k:
                break

        return results

    def _save_to_cache(self):
        """ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        print(f"\nğŸ’¾ ë²¡í„° ì¸ë±ìŠ¤ ìºì‹± ì¤‘...")

        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, str(self.index_path))

            # ë¬¸ì„œ ì €ì¥
            with open(self.docs_path, 'wb') as f:
                pickle.dump(self.documents, f)

            # ì„¤ì • ì €ì¥
            config = {
                'model_name': self.model_name,
                'dimension': self.dimension,
                'num_documents': len(self.documents)
            }
            with open(self.config_path, 'wb') as f:
                pickle.dump(config, f)

            print(f"âœ“ ìºì‹œ ì €ì¥ ì™„ë£Œ: {VECTOR_STORE_DIR}")

        except Exception as e:
            print(f"âœ— ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _load_from_cache(self) -> bool:
        """
        ë””ìŠ¤í¬ì—ì„œ ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not (self.index_path.exists() and self.docs_path.exists() and self.config_path.exists()):
            return False

        try:
            print(f"\nğŸ“‚ ìºì‹œëœ ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")

            # ì„¤ì • ë¡œë“œ
            with open(self.config_path, 'rb') as f:
                config = pickle.load(f)

            # ëª¨ë¸ ì¼ì¹˜ í™•ì¸
            if config['model_name'] != self.model_name:
                print(f"âš  ëª¨ë¸ ë¶ˆì¼ì¹˜: {config['model_name']} != {self.model_name}")
                return False

            # ëª¨ë¸ ë¡œë“œ
            self._load_model()

            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            self.index = faiss.read_index(str(self.index_path))

            # ë¬¸ì„œ ë¡œë“œ
            with open(self.docs_path, 'rb') as f:
                self.documents = pickle.load(f)

            self.dimension = config['dimension']

            print(f"âœ“ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.documents)} ë¬¸ì„œ, {self.index.ntotal} ë²¡í„°")
            return True

        except Exception as e:
            print(f"âœ— ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def clear_cache(self):
        """ìºì‹œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
        for path in [self.index_path, self.docs_path, self.config_path]:
            if path.exists():
                path.unlink()
        print("âœ“ ìºì‹œ ì‚­ì œ ì™„ë£Œ")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    from data_loader import DataLoader

    print("=== ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸ ===")

    # ë°ì´í„° ë¡œë“œ
    data_loader = DataLoader()
    documents = data_loader.load_all_data()

    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    vector_store = VectorStore()
    vector_store.build_index(documents, force_rebuild=True)

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_queries = [
        "í•œêµ­ì¼€ë¯¸ì¹¼ìƒì‚¬",
        "ìµœê·¼ ë§¤ì¶œ",
        "ê±°ë˜ì²˜ ì •ë³´"
    ]

    for query in test_queries:
        print(f"\nê²€ìƒ‰ì–´: {query}")
        results = vector_store.search(query, top_k=3)

        for i, (doc, score) in enumerate(results, 1):
            print(f"\n  {i}. [score={score:.4f}] {doc.metadata['source']}")
            print(f"     {doc.text[:200]}...")
